MAC安装hadoop
1 ssh-keygen -t rsa 
2 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys  (授权公钥到本地)
chmod 600 authorized_keys (授权)
chmod 700 -R .ssh
安装Hadoop
brew install hadoop
1 修改/usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoop里的hadoop-env.sh
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
改为
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
2 修改/usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoop里的core-site.xml
<configuration></configuration>
改为
<configuration>
  <property>
     <name>hadoop.tmp.dir</name>
<value>/usr/local/Cellar/hadoop/hdfs/tmp</value>
    <description>A base for other temporary directories.</description>
  </property>
  <property>
     <name>fs.default.name</name>
     <value>hdfs://localhost:8020</value>
  </property>
</configuration>
3 修改mapred-site.xml
<configuration></configuration>
改为
<configuration>
      <property>
        <name>mapred.job.tracker</name>
        <value>localhost:8021</value>
      </property>
</configuration>
4 修改hdfs-site.xml
<configuration>
   <property>
     <name>dfs.replication</name>
     <value>1</value>
    </property>
</configuration>

进入/usr/local/Cellar/hadoop/3.0.0/libexec/sbin
执行 vi ~/.bash_profile (配置环境变量)
export HADOOP_HOME=/usr/local/Cellar/hadoop/3.0.0/libexec
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
source ~/.bash_profile (配置生效)
执行 hdfs namenode -format （文件系统的初始化）
启动
./start-all.sh   
./stop-all.sh
访问HDSF服务
http://localhost:9870
访问YARN服务
http://localhost:8088

 hadoop fs -ls -R hdfs://localhost:8020/ 列出指定目录下的文件及文件夹
 hadoop fs -mkdir -p /spark/wc/input 创建目录
 hadoop fs -ls hdfs://localhost:8020/double_number.txt 查看目录
 hadoop fs -cat hdfs://localhost:8020/spark/wc/output1/p*;  查看文件
 hadoop fs -put words2.txt /spark/wc/input  从本地文件系统上传文件到HDFS
 sc.textFile("hdfs://localhost:8020/wc/input/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://localhost:8020/wc/output/")

ps:hadoop-3.0.0/etc/hadoop/log4j.properties末尾中追加
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR

MAC安装spark
执行 tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz (解压spark安装包)
执行 vi ~/.bash_profile 
export SPARK_HOME=/Users/youth.s/Documents/spark-2.4.0-bin-hadoop2.7
:$SPARK_HOME/bin:$SPARK_HOME/sbin
执行 source ~/.bash_profile 
执行 spark-shell

sc.textFile("hdfs://localhost/wc/input/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://localhost/wc/output/")



sc.textFile("hdfs://localhost:8020/spark/wc/input/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://localhost:8020/spark/wc/output")

sc.textFile("hdfs://myha01/spark/wc/input/words.txt").flatMap(_.split(" 
")).map((_,1)).reduceByKey(_+_).saveAsTextFile("hdfs://myha01/spark/wc/output")

MAC安装hive
 brew install hive
 mysql> create database metastore;
 mysql> create user 'hive'@'localhost' identified by '123456';
 mysql> grant select,insert,update,delete,alter,create,index,references on metastore.* to 'hive'@'localhost';
 mysql> flush privileges;

$ cd /usr/local/Cellar/hive/2.1.1/libexec/conf
$ cp hive-default.xml.template hive-site.xml
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost/metastore</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive(填上述mysql中创建的用户名)</value>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>123456(填上述mysql中创建的用户密码)</value>
</property>
<property>
  <name>hive.exec.local.scratchdir</name>
  <value>/tmp/hive</value>
</property>
<property>
  <name>hive.querylog.location</name>
  <value>/tmp/hive</value>
</property>
<property>
  <name>hive.downloaded.resources.dir</name>
  <value>/tmp/hive</value>
</property>
<property>
  <name>hive.server2.logging.operation.log.location</name>
  <value>/tmp/hive</value>
</property>

cp mysql-connector-java-5.1.42/mysql-connector-java-5.1.42-bin.jar /usr/local/Cellar/hive/3.1.1/libexec/lib/
schematool -initSchema -dbType mysql
mysql> show tables;
hive

hive>create database hive_test;
hive>show databases;
hive>use hive_test;
hive>create table student(id int, name string) row format delimited fields terminated by '\t';
hive>show create table student;
hive>show tables;
hive>load data local inpath '/Users/student.txt' into table hive_test.student;
hive>select * from hive_test.student;

hive --service metastore &

