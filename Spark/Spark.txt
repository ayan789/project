MAC安装hadoop
1 ssh-keygen -t rsa 
2 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys  (授权公钥到本地)
chmod 600 authorized_keys (授权)
chmod 700 -R .ssh
安装Hadoop
brew install hadoop
1 修改/usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoop里的hadoop-env.sh
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
改为
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="
2 修改/usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoop里的core-site.xml
<configuration></configuration>
改为
<configuration>
  <property>
     <name>hadoop.tmp.dir</name>
<value>/usr/local/Cellar/hadoop/hdfs/tmp</value>
    <description>A base for other temporary directories.</description>
  </property>
  <property>
     <name>fs.default.name</name>
     <value>hdfs://localhost:8020</value>
  </property>
</configuration>
3 修改mapred-site.xml
<configuration></configuration>
改为
<configuration>
      <property>
        <name>mapred.job.tracker</name>
        <value>localhost:8021</value>
      </property>
</configuration>
4 修改hdfs-site.xml
<configuration>
   <property>
     <name>dfs.replication</name>
     <value>1</value>
    </property>
</configuration>

进入/usr/local/Cellar/hadoop/3.0.0/libexec/sbin
执行 hdfs namenode -format （文件系统的初始化）
执行 vi ~/.bash_profile (配置环境变量)
export HADOOP_HOME=/usr/local/Cellar/hadoop/3.0.0/libexec
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
source ~/.bash_profile (配置生效)
启动
./start-all.sh   
./stop-all.sh
访问HDSF服务
http://localhost:9870
访问YARN服务
http://localhost:8088

ps:hadoop-3.0.0/etc/hadoop/log4j.properties末尾中追加
log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR

MAC安装spark
执行 tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz (解压spark安装包)
执行 vi ~/.bash_profile 
export SPARK_HOME=/Users/youth.s/Documents/spark-2.4.0-bin-hadoop2.7
:$SPARK_HOME/bin:$SPARK_HOME/sbin
执行 source ~/.bash_profile 
执行 spark-shell



